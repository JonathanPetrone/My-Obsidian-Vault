Our [[System 1 & 2|System 1]] unconsciously replaces hard questions with easier related questions, then answers those instead.

The danger is that you feel confident because the substitute you created was answerable, even when it doesn't actually address what you need to know. And you don't recognize the switch.

We have an internal compulsion to fabricate explanations even when we genuinely don't know the answer. The explanation feels true because it's coherent, not because it's accurate.

**Classic example - Market commentary:**
- Hard question: "Why did the market move?" (unknowable)
- Easy substitute: "What news fits a plausible story?" (always answerable)

The journalist isn't lying—they can't detect the switch.

%%Important%%
This tendency is the worst in regards to stock markets, geopolitics, and mechanism-based medical diagnosis—domains where causal signals are weak but compelling stories are easy to generate.

#### Source:
Superforecasters - P. Tetlock (ch 2)

