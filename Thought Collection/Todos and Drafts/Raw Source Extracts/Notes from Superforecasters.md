## Chapter 1 - An Optimistic Skeptic

We. may regard the present state of the universe as the effect of its past and the cause of its future. An intellect which at a certain moment would know all forces that set nature in motion, and all positions of all items of which nature is composed, if this intellect were also vast enough to submit these data to analysis, it would embrace in a single formula the movements of the greatest bodies of the universe and those of the tiniest atom; for such an intellect nothing would be uncertain and the future just like the past would be present before its eyes.

%%(Believing we can predict the future with complete information)%%

In one of history’s great ironies, scientists today know vastly more than their colleagues a century ago, and possess vastly more data-crunching power, but they are much less confident in the prospects for perfect predictability.

%%(Linked to earlier perspective)%%

But it is one thing to recognize the limits on predictability, and quite another to dismiss all prediction as an exercise in futility.

%%(Prediction is still useful)%%

So is reality clocklike or cloud-like? Is the future predictable or not? These are false dichotomies, the first of many we will encounter. We live in a world of clocks and clouds and a vast jumble of other metaphors. **Unpredictability and predictability coexist uneasily in the intricately interlocking systems that make up our bodies, our societies, and the cosmos. How predictable something is depends on what we are trying to predict, how far into the future, and under what circumstances**.

%%(It's complicated to make predictions)%%

Meteorologists know that better than anyone. They make large numbers of forecasts and routinely check their accuracy—which is why we know that one- and two-day forecasts are typically quite accurate while eight-day forecasts are not. With these analyses, meteorologists are able to sharpen their understanding of how weather works and tweak their models. Then they try again. Forecast, measure, revise. Repeat. It’s a never-ending process of incremental improvement that explains why weather forecasts are good and slowly getting better.

%%(check accuracy for incremental improvement to get better)%%

I’ll delve into details later, but let’s note two key conclusions that emerge from this research. One, foresight is real. Some people—people like Bill Flack—have it in spades. They aren’t gurus or oracles with the power to peer decades into the future, but they do have a real, measurable skill at judging how high-stakes events are likely to unfold three months, six months, a year, or a year and a half in advance. The other conclusion is what makes these superforecasters so good. It’s not really who they are. It is what they do. Foresight isn’t a mysterious gift bestowed at birth. It is the product of particular ways of thinking, of gathering information, of updating beliefs. These habits of thought can be learned and cultivated by any intelligent, thoughtful, determined person.

%%(Foresight can be learned)%%

As with the experts who had real foresight in my earlier research, what matters most is how the forecaster thinks.

%%(Foresight mindset)%%

In forecasting, as in other fields, we will continue to see human judgment being displaced—to the consternation of white-collar workers—but we will also see more and more syntheses, like “freestyle chess,” in which humans with computers compete as teams, the human drawing on the computer’s indisputable strengths but also occasionally overriding the computer. The result is a combination that can (sometimes) beat both humans and machines. To reframe the man-versus-machine dichotomy, combinations of Garry Kasparov and Deep Blue may prove more robust than pure-human or pure-machine approaches.

If Ferrucci is right—I suspect he is—we will need to blend computer-based forecasting and subjective judgment in the future. So it’s time we got serious about both.

%%(Computer + Human cooperation)%%

## Chapter 2 - Illusions of Knowledge

That’s the problem. Cochrane didn’t doubt the specialist and the specialist didn’t doubt his own judgment and so neither man considered the possibility that the diagnosis was wrong and neither thought it wise to wait for the pathologist’s report before closing the books on the life of Archie Cochrane.

%%(Jumping to conclusions without enough doubt)%%

Not until the twentieth century did the idea of randomized trial experiments, careful measurement, and statistical power take hold. “Is the application of the numerical method to the subject-matter of medicine a trivial and time-wasting ingenuity as some hold, or is it an important stage in the development of our art, as others proclaim it,” the Lancet asked in 1921. The British statistician Austin Bradford Hill responded emphatically that it was the latter, and laid out a template for modern medical investigation.

It was the absence of doubt—and scientific rigor—that made medicine unscientific and caused it to stagnate for so long.

%%(experiments, measurement + statistics)%%

It is natural to identify our thinking with the ideas, images, plans, and feelings that flow through consciousness. What else could it be? If I ask, “Why did you buy that car?” you can trot out reasons: “Good mileage. Cute style. Great price.” But you can only share thoughts by introspecting; that is, by turning your attention inward and examining the contents of your mind. And introspection can only capture a tiny fraction of the complex processes whirling inside your head—and behind your decisions. In describing how we think and decide, modern psychologists often deploy a dual-system model that partitions our mental universe into two domains. System 2 is the familiar realm of conscious thought. It consists of everything we choose to focus on. By contrast, System 1 is largely a stranger to us. It is the realm of automatic perceptual and cognitive operations—like those you are running right now to transform the print on this page into a meaningful sentence or to hold the book while reaching for a glass and taking a sip. We have no awareness of these rapid-fire processes but we could not function without them. We would shut down. The numbering of the two systems is not arbitrary. System 1 comes first. It is fast and constantly running in the background. 

%%(systems 1 + 2)%%

A defining feature of intuitive judgment is its insensitivity to. the quality of the evidence on which the judgment is based. It has to be that way. System 1 can only do its job of delivering strong conclusions at lightning speed if it never pauses to wonder whether the evidence at handis flawed or inadequate, or if there is better evidence elsewhere. It must treat the available evidence as reliable and sufficient. These tacit assumptions are so vital to System 1 that Kahneman gave them an ungainly but oddly memorable label: WYSIATI (What You See Is All There Is).14 Of course, System 1 can’t conclude whatever it wants.

%%(More details on system 1)%%

In celebrated research, Michael Gazzaniga designed a bizarre situation in which sane people did indeed have no idea why they were doing what they were doing. His test subjects were “split-brain” patients, meaning that the left and right hemispheres of their brains could not communicate with each other because the connection between them, the corpus callosum, had been surgically severed (traditionally as a treatment for severe epilepsy). These people are remarkably normal, but their condition allows researchers to communicate directly with only one hemisphere of their brain—by showing an image to only the left or right field of vision—without sharing the communication with the other hemisphere. It’s like talking to two different people. In this case, the left field of vision (which reports to the right hemisphere) was shown a picture of a snowstorm and the person was asked to point to the picture that related to it. So he quite reasonably pointed at the shovel. The right field of vision (which reports to the left hemisphere) was shown an image of a chicken claw—and the person was then asked why his hand was pointed at a shovel. The left hemisphere had no idea why. But the person didn’t say “I don’t know.” Instead, he made up a story: “Oh, that’s simple,” one patient said. “The chicken claw goes with the chicken, and you need a shovel to clean out the chicken shed.”15 This compulsion to explain arises with clocklike regularity every time a stock market closes and a journalist says something like “The Dow rose ninety-five points today on news that…” A quick check will often reveal that the news that supposedly drove the market came out well after the market had risen. But that minimal level of scrutiny is seldom applied. It’s a rare day when a journalist says, “The market rose today for any one of a hundred different reasons, or a mix of them, so no one knows.” Instead, like a split-brain patient asked why he is pointing at a picture of a shovel when he has no idea why, the journalist conjures a plausible story from whatever is at hand.

%%(split-brain patients making things up as reasons, example of stock market analysts)%%

Like everyone else, scientists have intuitions. Indeed, hunches and flashes of insight—the sense that something is true even if you can’t prove it—have been behind countless breakthroughs. The interplay between System 1 and System 2 can be subtle and creative. But scientists are trained to be cautious. They know that no matter how tempting it is to anoint a pet hypothesis as The Truth, alternative explanations must get a hearing. And they must seriously consider the possibility that their initial hunch is wrong. In fact, in science, the best evidence that a hypothesis is true is often an experiment designed to prove the hypothesis is false, but which fails to do so. Scientists must be able to answer the question “What would convince me I am wrong?” If they can’t, it’s a sign they have grown too attached to their beliefs. The key is doubt. Scientists can feel just as strongly as anyone else that they know The Truth. But they know they must set that feeling aside and replace it with finely measured degrees of doubt—doubt that can be reduced (although never to zero) by better evidence from better studies.

%%(doubt and system 1 + 2)%%

Archie Cochrane’s skeptical defenses folded because Cochrane found the specialist’s story as intuitively compelling as the specialist did. But another mental process was likely at work, as well. Formally, it’s called attribute substitution, but I call it bait and switch: when faced with a hard question, we often surreptitiously replace it with an easy one. “Should I worry about the shadow in the long grass?” is a hard question. Without more data, it may be unanswerable. So we substitute an easier question: “Can I easily recall a lion attacking someone from the long grass?” That question becomes a proxy for the original question and if the answer is yes to the second question, the answer to the first also becomes yes. So the availability heuristic—like Kahneman’s other heuristics—is essentially a bait-and-switch maneuver. And just as the availability heuristic is usually an unconscious System 1 activity, so too is bait and switch.

%%(when faced with a hard question, we often surreptitiously replace it with an easy one)%%

While Daniel Kahneman and Amos Tversky were documenting System 1’s failings, another psychologist, Gary Klein, was examining decision making among professionals like the commanders of firefighting teams, and discovering that snap judgments can work astonishingly well.

There is nothing mystical about an accurate intuition like the fire commander’s. It’s pattern recognition. With training or experience, people can encode patterns deep in their memories in vast number and intricate detail—such as the estimated fifty thousand to one hundred thousand chess positions that top players have in their repertoire.20 If something doesn’t fit a pattern—like a kitchen fire giving off more heat than a kitchen fire should—a competent expert senses it immediately. But as we see every time someone spots the Virgin Mary in burnt toast or in mold on a church wall, our pattern-recognition ability comes at the cost of susceptibility to false positives. This, plus the many other ways in which the tip-of-your-nose perspective can generate perceptions that are clear, compelling, and wrong, means intuition can fail as spectacularly as it can work. **Whether intuition generates delusion or insight depends on whether you work in a world full of valid cues you can unconsciously register for future use.**

%%(Fire commanders dealing with complex situations due to pattern recognition)%%

All too often, forecasting in the twenty-first century looks too much like nineteenth-century medicine. There are theories, assertions, and arguments. There are famous figures, as confident as they are well compensated. But there is little experimentation, or anything that could be called science, so we know much less than most people realize. And we pay the price. Although bad forecasting rarely leads as obviously to harm as does bad medicine, it steers us subtly toward bad decisions and all that flows from them—including monetary losses, missed opportunities, unnecessary suffering, even war and death.

%%(Forecasting in twenty-first century )%%

## Chapter 3 - Keeping Store

But it’s not nearly so simple. Consider a forecast Steve Ballmer made in 2007, when he was CEO of Microsoft: “There’s no chance that the iPhone is going to get any significant market share. No chance.”

But parse Ballmer’s forecast carefully. The key term is “significant market share.” What qualifies as “significant”? Ballmer didn’t say. And which market was he talking about? North America? The world? And the market for what? Smartphones or mobile phones in general? All these unanswered questions add up to a big problem. The first step in learning what works in forecasting, and what doesn’t, is to judge forecasts, and to do that we can’t make assumptions about what the forecast means. We have to know. There can’t be any ambiguity about whether a forecast is accurate or not and Ballmer’s forecast is ambiguous. Sure, it looks wrong. It feels wrong. There is a strong case to be made that it is wrong. But is it wrong beyond all reasonable doubt?

This problem alone renders many everyday forecasts untestable. Similarly, forecasts often rely on implicit understandings of key terms rather than explicit definitions—like “significant market share” in Steve Ballmer’s forecast.

%% Uncertainty what is actually predicted to happen, and when%%

In intelligence circles, Sherman Kent is a legend. With a PhD in history, Kent left a faculty position at Yale to join the Research and Analysis Branch of the newly created Coordinator of Information (COI) in 1941. The COI became the Office of Strategic Services (OSS). The OSS became the Central Intelligence Agency (CIA). By the time Kent retired from the CIA in 1967, he had profoundly shaped how the American intelligence community does what it calls intelligence analysis—the methodical examination of the information collected by spies and surveillance to figure out what it means, and what will happen next. The key word in Kent’s work is estimate. As Kent wrote, “estimating is what you do when you do not know.”9 And as Kent emphasized over and over, we never truly know what will happen next. Hence forecasting is all about estimating the likelihood of something happening, which Kent and his colleagues did for many years at the Office of National Estimates—

%% Estimations, rather than prediction in forecasting%%

Disturbed, Kent went back to his team. They had all agreed to use “serious possibility” in the NIE so Kent asked each person, in turn, what he thought it meant. One analyst said it meant odds of about 80 to 20, or four times more likely than not that there would be an invasion. Another thought it meant odds of 20 to 80—exactly the opposite.

%%Meaning and interpretation of words, serious possibility%%

Sherman Kent suggested a solution.

![[Skärmavbild 2026-01-12 kl. 17.30.48.png]]

But it was never adopted. People liked clarity and precision in principle but when it came time to make clear and precise forecasts they weren’t so keen on numbers. Some said it felt unnatural or awkward, which it does when you’ve spent a lifetime using vague language, but that’s a weak argument against change.

A more serious objection—then and now—is that expressing a probability estimate with a number may imply to the reader that it is an objective fact, not the subjective judgment it is.

So what’s the safe thing to do? Stick with elastic language. Forecasters who use “a fair chance” and “a serious possibility” can even make the wrong-side-of-maybe fallacy work for them: If the event happens, “a fair chance” can retroactively be stretched to mean something considerably bigger than 50%

%% On assigning probablities%%

If we are serious about measuring and improving, this won’t do. Forecasts must have clearly defined terms and timelines. They must use numbers. And one more thing is essential: we must have lots of forecasts.

%% Necessity of defined terms and timelines%%

Not quite. Remember that the whole point of this exercise is to judge the accuracy of forecasts so we can then figure out what works in forecasting and what doesn’t. To do that, we have to interpret the meaning of the Brier scores, which requires two more things: benchmarks and comparability. Let’s suppose we discover that you have a Brier score of 0.2. That’s far from godlike omniscience (0) but a lot better than chimp-like guessing (0.5), so it falls in the range of what one might expect from, say, a human being. But we can say much more than that. What a Brier score means depends on what’s being forecast. For instance, it’s quite easy to imagine circumstances where a Brier score of 0.2 would be disappointing. Consider the weather in Phoenix, Arizona. Each June, it gets very hot and sunny. A forecaster who followed a mindless rule like, “always assign 100% to hot and sunny” could get a Brier score close to 0, leaving 0.2 in the dust. Here, the right test of skill would be whether a forecaster can do better than mindlessly predicting no change. This is an underappreciated point. For example, after the 2012 presidential election, Nate Silver, Princeton’s Sam Wang, and other poll aggregators were hailed for correctly predicting all fifty state outcomes, but almost no one noted that a crude, across-the-board prediction of “no change”—if a state went Democratic or Republican in 2008, it will do the same in 2012—would have scored forty-eight out of fifty, which suggests that the many excited exclamations of “He called all fifty states!” we heard at the time were a tad overwrought. Fortunately, poll aggregators are pros: they know that improving predictions tends to be a game of inches.

%% Forecasts and Brier Scores
%%

In the EPJ results, there were two statistically distinguishable groups of experts. The first failed to do better than random guessing, and in their longer-range forecasts even managed to lose to the chimp. The second group beat the chimp, though not by a wide margin, and they still had plenty of reason to be humble. Indeed, they only barely beat simple algorithms like “always predict no change” or “predict the recent rate of change.” Still, however modest their foresight was, they had some.

So why did one group do better than the other? It wasn’t whether they had PhDs or access to classified information. Nor was it what they thought—whether they were liberals or conservatives, optimists or pessimists. The critical factor was how they thought.

I just liked the metaphor because it captured something deep in my data. I dubbed the Big Idea experts “hedgehogs” and the more eclectic experts “foxes.” Foxes beat hedgehogs. And the foxes didn’t just win by acting like chickens, playing it safe with 60% and 70% forecasts where hedgehogs boldly went with 90% and 100%. Foxes beat hedgehogs on both calibration and resolution. Foxes had real foresight. Hedgehogs didn’t.

%%One vs. many perspectives in forecasting%%

But the hedgehog also “knows one big thing,” the Big Idea he uses over and over when trying to figure out what will happen next. Think of that Big Idea like a pair of glasses that the hedgehog never takes off.

Foxes don’t fare so well in the media. They’re less confident, less likely to say something is “certain” or “impossible,” and are likelier to settle on shades of “maybe.” And their stories are complex, full of “howevers” and “on the other hands,” because they look at problems one way, then another, and another. This aggregation of many perspectives is bad TV. But it’s good forecasting. Indeed, it’s essential.

%%One vs. many perspectives in forecasting - on hedgehogs vs. foxes%%

In 1906 the legendary British scientist. Sir Francis Galton went to a country fair and watched as hundreds of people individually guessed the weight that a live ox would be after it was “slaughtered and dressed.” Their average guess—their collective judgment—was 1,197 pounds, one pound short of thecorrect answer, 1,198 pounds. It was the earliest demonstration of a phenomenon popularized by—and now named for—James Surowiecki’s bestseller The Wisdom of Crowds. Aggregating the judgment of many consistently beats the accuracy of the average member of the group, and is often as startlingly accurate as Galton’s weight-guessers.

%% wisdom of crowds ?%%

How well aggregation works. depends on what you are aggregating. Aggregating the judgments of many people who know nothing produces a lot of nothing. Aggregating the judgments of people who know a little is better, and if there are enough of them, it can produce impressive results, but aggregating the judgments of an equal number ofpeople who know lots about lots of different things is most effective because the collective pool of information becomes much bigger.

Now look at how foxes approach forecasting. They deploy not one analytical idea but many and seek out information not from one source but many. Then they synthesize it all into a single conclusion. In a word, they aggregate. They may be individuals working alone, but what they do is, in principle, no different from what Galton’s crowd did. They integrate perspectives and the information contained within them. The only real difference is that the process occurs within one skull.

%% aggregating with useful knowledge -> fox perspective%%

Like us, dragonflies have two eyes, but theirs are constructed very differently. Each eye is an enormous, bulging sphere, the surface of which is covered with tiny lenses. Depending on the species, there may be as many as thirty thousand of these lenses on a single eye, each one occupying a physical space slightly different from those of the adjacent lenses, giving it a unique perspective. Information from these thousands of unique perspectives flows into the dragonfly’s brain where it is synthesized into vision so superb that the dragonfly can see in almost every direction simultaneously, with the clarity and precision it needs to pick off flying insects at high speed.

A fox with the bulging eyes of a dragonfly is an ugly mixed metaphor but it captures a key reason why the foresight of foxes is superior to that of hedgehogs with their green-tinted glasses. Foxes aggregate perspectives.

%% Dragonfly eyes and a fox%%

My fox/hedgehog model is not a dichotomy. It is a spectrum. In EPJ, my analyses extended to what I called “hybrids”—“fox-hedgehogs,” who are foxes a little closer to the hedgehog end of the spectrum, and “hedgehog-foxes,” who are hedgehogs with a little foxiness.

%% The model fox/hedgehog is not a dichotomy%%
## Chapter 4 - Superforecasters

What there is instead is accountability for process: Intelligence analysts are told what they are expected to do when researching, thinking, and judging, and then held accountable to those standards. Did you consider alternative hypotheses? Did you look for contrary evidence? It’s sensible stuff, but the point of making forecasts is not to tick all the boxes on the “how to make forecasts” checklist. It is to foresee what’s coming. To have accountability for process but not accuracy is like ensuring that physicians wash their hands, examine the patient, and consider all the symptoms, but never checking to see whether the treatment works.

%%There should be accountability for process not outcome%%

Here’s one possible revelation: Imagine you get a couple of hundred ordinary people to forecast geopolitical events. You see how often they revise their forecasts and how accurate those forecasts prove to be and use that information to identify the forty or so who are the best. Then you have everyone make lots more forecasts. This time, you calculate the average forecast of the whole group—“the wisdom of the crowd”—but with extra weight given to those forty top forecasters. Then you give the forecast a final tweak: You “extremize” it, meaning you push it closer to 100% or zero. If the forecast is 70% you might bump it up to, say, 85%. If it’s 30%, you might reduce it to 15%.9 Now imagine that the forecasts you produce this way beat those of every other group and method available, often by large margins. Your forecasts even beat those of professional intelligence analysts inside the government who have access to classified information—by margins that remain classified.

It actually happened. What I’ve described is the method we used to win IARPA’s tournament. There is nothing dazzlingly innovative about it. Even the extremizing tweak is based on a pretty simple insight: When you combine the judgments of a large group of people to calculate the “wisdom of the crowd” you collect all the relevant information that is dispersed among all those people. But none of those people has access to all that information. One person knows only some of it, another knows some more, and so on.

%%wisdom of the crowd knowledge spread%%

They were our first class of superforecasters. At the end of year 1, their collective Brier score was 0.25, compared with 0.37 for all the other forecasters—and that gap grew in later years so that by the end of the four-year tournament, superforecasters had outperformed regulars by over 60%. Another gauge of how good superforecasters were is how much further they could see into the future. Across all four years of the tournament, superforecasters looking out three hundred days were more accurate than regular forecasters looking out one hundred days. In other words, regular forecasters needed to triple their foresight to see as far as superforecasters.

%%superforecasters excellence%%

A variant of this fallacy is to single out an extraordinarily successful person, show that it was extremely unlikely that the person could do what he or she did, and conclude that luck could not be the explanation. This often happens in news coverage of Wall Street. Someone beats the market six or seven years in a row, journalists profile the great investor, calculate how unlikely it is to get such results by luck alone, and triumphantly announce that it’s proof of skill. The mistake?

A similar mistake can be found by rummaging through remaindered business books: a corporation or executive is on a roll, going from success to success, piling up money and fawning profiles in magazines. What comes next? Inevitably, it’s a book recounting the successes and assuring readers that they can reap similar successes simply by doing whatever the corporation or executive did. These stories may be true—or fairy tales. It’s impossible to know. These books seldom provide solid evidence that the highlighted qualities or actions caused happy outcomes, much less that someone who replicates them will get similarly happy outcomes.

%%hard to mirror successful results/predictions, what is luck and not?%%

Keep regression to the mean in mind, however, and it becomes a valuable tool. Imagine that we had our 2,800 volunteers predict the outcome of 104 coin tosses a second time. The distribution would again look like a bell curve, with most people clustered around 50% and tiny numbers correctly predicting almost none or almost all. But who gets the amazingly good results this time? Most likely it will be different people from last time. The correlation across rounds will be close to zero, and your best prediction for any given forecaster will be the average accuracy rate of 50%—in other words, total regression to the mean.

So regression to the mean is an indispensable tool for testing the role of luck in performance: Mauboussin notes that slow regression is more often seen in activities dominated by skill, while faster regression is more associated with chance.

But, as Wall Streeters well know, mortals can only defy the laws of statistical gravity for so long. The consistency in performance of superforecasters as a group should not mask the inevitable turnover among some top performers over time.

But that also implies a good deal of consistency over time: 70% of superforecasters remain superforecasters.

All of this suggests two key conclusions. One, we should not treat the superstars of any given year as infallible, not even Doug Lorch. Luck plays a role and it is only to be expected that the superstars will occasionally have a bad year and produce ordinary results—just as superstar athletes occasionally look less than stellar. But more basically, and more hopefully, we can conclude that the superforecasters were not just lucky. Mostly, their results reflected skill. Which raises the big question: Why are superforecasters so good?

%% Regression to the mean is a effective tool for evaluating performance%%

## Chapter 5 - Supersmart?

What did we find? Regular forecasters scored higher on intelligence and knowledge tests than about 70% of the population. Superforecasters did better, placing higher than about 80% of the population.

But having the requisite intelligence and knowledge is not enough. Many clever and informed forecasters in the tournament fell far short of superforecaster accuracy. And history is replete with brilliant people who made forecasts that proved considerably less than prescient.

%%Intelligence and knowledge is not everything but a good contributor%%

Here’s a question that definitely was not asked in the forecasting tournament: How many piano tuners are there in Chicago? Don’t even think about letting Google find the answer for you. The Italian American physicist Enrico Fermi—a central figure in the invention of the atomic bomb—concocted this little brainteaser decades before the invention of the Internet. And Fermi’s students did not have the Chicago yellow pages at hand. They had nothing. And yet Fermi expected them to come up with a reasonably accurate estimate.

So I could nail this question if I knew four facts: 
1. The number of pianos in Chicago
2. How often pianos are tuned each year
3. How long it takes to tune a piano
4. How many hours a year the average piano tuner works

Not so. What Fermi understood is that by breaking down the question, we can better separate the knowable and the unknowable. So guessing—pulling a number out of the black box—isn’t eliminated. But we have brought our guessing process out into the light of day where we can inspect it. And the net result tends to be a more accurate estimate than whatever number happened to pop out of the black box when we first read the question. Of course, all this means we have to overcome our deep-rooted fear of looking dumb. Fermi-izing dares us to be wrong. In that spirit, 

%%Fermi-mizing method%%

Read the question again: “Will either the French or Swiss inquiries find elevated levels of polonium in the remains of Yasser Arafat’s body?” Neither “Israel would never do that!” nor “Of course Israel did it!” is actually responsive to that question. They answer a different question: “Did Israel poison Yasser Arafat?” System 1 pulled a classic bait and switch: the hard question that was actually asked was replaced by the easy question that wasn’t. That trap could have been avoided. The key is to Fermi-ize.

%%Hidden assumptions when answering, could be somoene else poisoning Arafat%% 

My question: How likely is it that the Renzettis have a pet? To answer that, most people would zero in on the family’s details. “Renzetti is an Italian name,” someone might think. “So are ‘Frank’ and ‘Camila.’ That may mean Frank grew up with lots of brothers and sisters, but he’s only got one child. He probably wants to have a big family but he can’t afford it. So it would make sense that he compensated a little by getting a pet.” Someone else might think, “People get pets for kids and the Renzettis only have one child, and Tommy isn’t old enough to take care of a pet. So it seems unlikely.” This sort of storytelling can be very compelling, particularly when the available details are much richer than what I’ve provided here. But superforecasters wouldn’t bother with any of that, at least not at first. The first thing they would do is find out what percentage of American households own a pet. Statisticians call that the base rate—how common something is within a broader class. Daniel Kahneman has a much more evocative visual term for it. He calls it the “outside view”—in contrast to the “inside view,” which is the specifics of the particular case. A few minutes with Google tells me about 62% of American households own pets. That’s the outside view here. Starting with the outside view means means I will start by estimating that there is a 62% chance the Renzettis have a pet. 

Superforecasters don’t make that mistake. If Bill Flack were asked whether, in the next twelve months, there would be an armed clash between China and Vietnam over some border dispute, he wouldn’t immediately delve into the particulars of that border dispute and the current state of China-Vietnam relations. He would instead look at how often there have been armed clashes in the past. “Say we get hostile conduct between China and Vietnam every five years,” Bill says. “I’ll use a five-year recurrence model to predict the future.” In any given year, then, the outside view would suggest to Bill there is a 20% chance of a clash. Having established that, Bill would look at the situation today and adjust that number up or down.

%%Outside view vs inside view%%

When we make estimates, we tend to start with some number and adjust. The number we start with is called the anchor. It’s important because we typically underadjust, which means a bad anchor can easily produce a bad estimate. And it’s astonishingly easy to settle on a bad anchor.

So a forecaster who starts by diving into the inside view risks being swayed by a number that may have little or no meaning. But if she starts with the outside view, her analysis will begin with an anchor that is meaningful. And a better anchor is a distinct advantage.

%% A good statistic is a good anchor%%

THE INSIDE VIEW You’ve Fermi-ized the question, consulted the outside view, and now, finally, you can delve into the inside view. In the case of the Arafat-polonium question, that means digging into Middle Eastern politics and history. And there’s a lot of that. So you fill a small library with books and settle in for six months of reading. Right? Wrong. Such diligence would be admirable, but it would also be misguided. If you aimlessly examine one tree, then another, and another, you will quickly become lost in the forest. A good exploration of the inside view does not involve wandering around, soaking up any and all information and hoping that insight somehow emerges. It is targeted and purposeful: it is an investigation, not an amble.11 Again, Fermi-ization is key. When Bill Flack Fermi-ized the Arafat-polonium question, he realized there were several pathways to a “yes” answer: Israel could have poisoned Arafat; Arafat’s Palestinian enemies could have poisoned him; or Arafat’s remains could have been contaminated after his death to make it look like a poisoning. Hypotheses like these are the ideal framework for investigating the inside view.

%% Fermi-ization of inside view%%

Coming up with an outside view, an inside view, and a synthesis of the two isn’t the end. It’s a good beginning. Superforecasters constantly look for other views they can synthesize into their own. There are many different ways to obtain new perspectives. What do other forecasters think? What outside and inside views have they come up with? What are experts saying? You can even train yourself to generate different perspectives. When Bill Flack makes a judgment, he often explains his thinking to his teammates, as David Rogg did, and he asks them to critique it. In part, he does that because he hopes they’ll spot flaws and offer their own perspectives. But writing his judgment down is also a way of distancing himself from it, so he can step back and scrutinize it: “It’s an auto-feedback thing,” he says. “Do I agree with this? Are there holes in this? Should I be looking for something else to fill this in? Would I be convinced by this if I were somebody else?”

There is an even simpler way of getting another perspective on a question: tweak its wording.

%% Perspectives to synthesize after outside and inside view%%

An element of personality is also likely involved. In personality psychology, one of the “Big Five” traits is “openness to experience,” which has various dimensions, including preference for variety and intellectual curiosity. It’s unmistakable in many superforecasters.

%%Openness to expierence is present in superforecasters%%

For superforecasters, beliefs are hypotheses to be tested, not treasures to be guarded. It would be facile to reduce superforecasting to a bumper-sticker slogan, but if I had to, that would be it.

%% beliefs are hypotheses to be tested %%
## Chapter 6 - Superquants?

Levine is also a superforecaster. And while he is an extreme case, he underscores a central feature of superforecasters: they have a way with numbers. Most aced a brief test of basic numeracy that asked questions like, “The chance of getting a viral infection is 0.05%. Out of 10,000 people, about how many of them are expected to get infected?” (The answer is 5.) And numeracy is just as evident on their résumés. Many have backgrounds in math, science, or computer programming.

%% Many superforecasters have backgrounds in math, science etc.%%

But the fact that superforecasters are almost uniformly highly numerate people is not mere coincidence. Superior numeracy does help superforecasters, but not because it lets them tap into arcane math models that divine the future. The truth is simpler, subtler, and much more interesting.

%%numeracy helps forecasters%%

The real Leon Panetta understands process-outcome paradoxes like this. And he is much less keen on certainty than the fictional Leon Panetta. “Nothing is one hundred percent,” he said several times during our interview. The real Leon Panetta thinks like a superforecaster.3

%%nothing is ever 100% %%

Harry Truman once joked that he wanted to hear from a one-armed economist because. he was sick of hearing “on the one hand…on the other…”—a joke that bears more than a passing resemblance to Tversky’s. We want answers. A confident yes or no is satisfying in a way that maybe never is, a fact that helps to explain why the media so often turn to hedgehogs who are sure they know what is coming no matter how bad their forecasting recordsmay be.

%% we want answers %% 

Confidence and accuracy are positively correlated. But research shows we exaggerate the size of the correlation. For instance, people trust more confident financial advisers over those who are less confident even when their track records are identical.

%% overstimation of correlation of confidence and accuracy%%

Scientists come at probability in a radically different way. They relish uncertainty, or at least accept it, because in scientific models of reality, certainty is illusory. Leon Panetta may not be a scientist, but he captured that insight perfectly when he said, “Nothing is one hundred percent.” That may be a little surprising. “Most people would identify science with certainty,” wrote the mathematician and statistician William Byers.

In the popular mind, scientists generate facts and chisel them into granite tablets. This collection of facts is what we call “science.” As the work of accumulating facts proceeds, uncertainty is pushed back. The ultimate goal of science is uncertainty’s total eradication. But that is a very nineteenth-century view of science. One of twentieth-century science’s great accomplishments has been to show that uncertainty is an ineradicable element of reality. “Uncertainty is real,” Byers writes. “It is the dream of total certainty that is an illusion.”15 This is true both at the margin of scientific knowledge and at what currently appears to be its core. Scientific facts that look as solid as rock to one generation of scientists can be crushed to dust beneath the advances of the next.16 All scientific knowledge is tentative. Nothing is chiseled in granite.

%% Science is never 100% certainty %%

Now, I have urged the reader to be skeptical and a skeptic may have doubts about this. It’s easy to impress people by stroking your chin and declaring “There is a 73% probability Apple’s stock will finish the year 24% above where it started.” Toss in a few technical terms most people don’t understand—“stochastic” this, “regression” that—and you can use people’s justified respect for math and science to get them nodding along. This is granularity as bafflegab. It is unfortunately common. So how can we know that the granularity we see among superforecasters is meaningful? How can we be sure that when Brian Labatte makes an initial estimate of 70% but then stops himself and adjusts it to 65% the change is likely to produce a more accurate estimate? The answer lies in the tournament data. Barbara Mellers has shown that granularity predicts accuracy: the average forecaster who sticks with the tens—20%, 30%, 40%—is less accurate than the finer-grained forecaster who uses fives—20%, 25%, 30%—and still less accurate than the even finer-grained forecaster who uses ones—20%, 21%, 22%. As a further test, she rounded forecasts to make them less granular, so a forecast at the greatest granularity possible in the tournament, single percentage points, would be rounded to the nearest five, and then the nearest ten. This way, all of the forecasts were made one level less granular. She then recalculated Brier scores and discovered that superforecasters lost accuracy in response to even the smallest-scale rounding, to the nearest 0.05, whereas regular forecasters lost little even from rounding four times as large, to the nearest 0.2.22

%% More granularity is preferred in predicitons%%

Natural as such thinking may be, it is problematic. Lay out the tangled chain of reasoning in a straight line and you see this: “The probability that I would meet the love of my life was tiny. But it happened. So it was meant to be. Therefore the probability that it would happen was 100%.” This is beyond dubious. It’s incoherent. Logic and psycho-logic are in tension. A probabilistic thinker will be less distracted by “why” questions and focus on “how.” This is no semantic quibble. “Why?” directs us to metaphysics; “How?” sticks with physics. The probabilistic thinker would say, “Yes, it was extremely improbable that I would meet my partner that night, but I had to be somewhere and she had to be somewhere and happily for us our somewheres coincided.”

%% Challenging assumptions of it "had to be" %%

For both the superforecasters and the regulars, we also compared individual fate scores with Brier scores and found a significant correlation—meaning the more a forecaster inclined toward it-was-meant-to-happen thinking, the less accurate her forecasts were. Or, put more positively, the more a forecaster embraced probabilistic thinking, the more accurate she was. So finding meaning in events is positively correlated with wellbeing but negatively correlated with foresight. That sets up a depressing possibility: Is misery the price of accuracy?

%% Meaning is negatively correlated with foresight %%

## Chapter 7 - Supernewsjunkies?

**Superforecasting isn’t a paint-by-numbers method but superforecasters often tackle questions in a roughly similar way—one that any of us can follow: Unpack the question into components. Distinguish as sharply as you can between the known and unknown and leave no assumptions unscrutinized. Adopt the outside view and put the problem into a comparative perspective that downplays its uniqueness and treats it as a special case of a wider class of phenomena. Then adopt the inside view that plays up the uniqueness of the problem. Also explore the similarities and differences between your views and those of others—and pay special attention to prediction markets and other methods of extracting wisdom from crowds. Synthesize all these different views into a single vision as acute as that of a dragonfly. Finally, express your judgment as precisely as you can, using a finely grained scale of probability.**

%%The process of superforecasting%%

Forecasts aren’t like lottery tickets that you buy and file away until the big draw. They are judgments that are based on available information and that should be updated in light of changing information.

This is obviously important. A forecast that is updated to reflect the latest available information is likely to be closer to the truth than a forecast that isn’t so informed.

%% change/update predictions %%

Superforecasters do monitor the news carefully and factor it into. their forecasts, which is bound to give them a big advantage over the less attentive. If that’s the decisive factor, then superforecasters’ success would tell us nothing more than “it helps to pay attention and keep your forecast up to date”—which is about as enlightening as being told that when polls show a candidate surging into a comfortable leadhe is more likely to win. But that’s not the whole story. For one thing, superforecasters’ initial forecasts were at least 50% more accurate than those of regular forecasters.

%% Attention and careful initial forecasts%%

But that was only one possible explanation, so Bill cautiously raised his forecast to 65% yes. That’s smart updating.

%% update when it is more likely etc.%%

But there is a subtler explanation for Bill Flack’s underreaction to a Japanese official saying Shinzo Abe would visit the Yasukuni Shrine. The political costs of visiting Yasukuni were steep. And Abe had no pressing need to placate his conservative constituents by going, so the benefit was negligible. Conclusion? Not going looked like the rational decision. But Bill ignored Abe’s own feelings. Abe is a conservative nationalist. He had visited Yasukuni before, although not as prime minister. He wanted to go again. Reflecting on his mistake, Bill told me, “I think that the question I was really answering wasn’t ‘Will Abe visit Yasukuni?’ but ‘If I were PM of Japan, would I visit Yasukuni?’

%% Make sure you are answering the right question %%

This is an extreme case of what psychologists call “belief perseverance.” People can be astonishingly intransigent—and capable of rationalizing like crazy to avoid acknowledging new information that upsets their settled beliefs.

“The very fact that no sabotage has taken place to date is a disturbing and confirming indication that such action will be taken.”6—or to put that more bluntly, “The fact that what I expected to happen didn’t happen proves that it will.”

%% belief perserverance%%

This suggests that superforecasters may have a surprising advantage: they’re not experts or professionals, so they have little ego invested in each forecast. Except in rare circumstances—when Jean-Pierre Beugoms answers military questions, for example—they aren’t deeply committed to their judgments, which makes it easier to admit when a forecast is offtrack and adjust.

%% less investment in pride is good for outcome %%

Now try this one: David is a psychotherapy patient who is sexually aroused by violent sadomasochistic fantasies. Question: How likely is David to be a child abuser?

You may be thinking, “All that additional information is irrelevant. I’d ignore it.” And good for you. It was carefully preselected for its total irrelevance. And yet, irrelevant information of this sort does sway us. In 1989, building on work by the psychologist Richard Nisbett, I ran a study in which randomly selected participants got either the bare minimum facts or that information plus the irrelevant facts and then estimated Robert’s GPA or David’s proclivity for child abuse. As expected, those who got the irrelevant information lost confidence.

Psychologists call this the dilution effect, and given that stereotypes are themselves a source of bias we might say that diluting them is all to the good. Yes and no. Yes, it is possible to fight fire with fire, and bias with bias, but the dilution effect remains a bias. Remember what’s going on here. People base their estimate on what they think is a useful tidbit of information.

Look at a typical day in the stock market. The volume and volatility of trading are staggering. The reasons for that are complex and the subject of much research and debate, but it seems clear that at least some of it is due to traders overreacting to new information.

%% Make judgment on the information%%

As with underreaction, the key is commitment—in this case, the absence of commitment. Traders who constantly buy and sell are not cognitively or emotionally connected to their stocks.

%% commitment to beliefs%%

Given superforecasters’ modest commitment to their forecasts, we would expect overreactions—like that of Doug Lorch and his team swinging from 55% to 95% on a single, month-old report—to be a greater risk than underreactions.

Forecasters should feel the same about under- and overreaction to new information, the Scylla and Charybdis of forecasting. Good updating is all about finding the middle passage.

%% the key to good forecasting is not to overreact or underreact to new information%%

There are no dramatic swings of thirty or forty percentage points. The average update was tiny, only 3.5%. That was critical. A few small updates would have put Tim on a heading for underreaction. Many large updates could have tipped him toward overreaction. But with many small updates, Tim slipped safely between Scylla and Charybdis.

%%On Tim (superforecaster) updates on forecast, modest changes but frequent%%

But the forecaster who carefully balances old and new captures the value in both—and puts it into her new forecast. The best way to do that is by updating often but bit by bit.

and

That essay, in combination with the work of Bayes’ friend Richard Price, who published Bayes’ essay posthumously in 1761, and the insights of the great French mathematician Pierre-Simon Laplace, ultimately produced Bayes’ theorem. It looks like this: P(H|D)/P(-H|D) = P(D|H) • P(D|-H) • P(H)/P(-H) Posterior Odds = Likelihood Ratio • Prior Odds The Bayesian belief-updating equation In simple terms, the theorem says that your new belief should depend on two things—your prior belief (and all the knowledge that informed it) multiplied by the “diagnostic value” of the new information.

%% bayes theorem and updating often%%

But then Hagel botched the hearing. Clearly, that reduced his chances. But by how much? To answer that, Ulfelder wrote, “Bayes’ theorem requires us to estimate two things: 1) how likely are we to see a poor Senate performance when the nominee is destined to fail and 2) how likely are we to see a poor performance when the nominee is bound for approval?”

“For the sake of argument, I’ll assume that only one of every five nominees bound for success does poorly in confirmation hearings, but 19 of 20 bound for failure do.” Ulfelder plugged the numbers into Bayes’ formula, did the math, and his forecast “tumbles from 96 percent to a lowly…83 percent.” Thus, Ulfelder concluded Ricks’s estimate was way off and Hagel was still very likely to be confirmed. And he was, two weeks later.18 This may cause the math-averse to despair. Do forecasters really have to understand, memorize, and use a—shudder—algebraic formula? For you, I have good news: no, you don’t. The superforecasters are a numerate bunch: many know about Bayes’ theorem and could deploy it if they felt it was worth the trouble.

%%example of bayes theorem%%

And yet Minto appreciates the Bayesian spirit. “I think it is likely that I have a better intuitive grasp of Bayes’ theorem than most people,” he said, “even though if you asked me to write it down from memory I’d probably fail.” Minto is a Bayesian who does not use Bayes’ theorem. That paradoxical description applies to most superforecasters.

%%on capturing the essence of bayes theorem%%

Superforecasters understand the principles but also know that their application requires nuanced judgments. And they would rather break the rules than make a barbarous forecast.

## Chapter 8 - Perpetual Beta

To be a top-flight forecaster, a growth mindset is essential. The best illustration is the man who is reputed to have said—but didn’t—“When the facts change, I change my mind.”

%% growth mindset importance of%%

We learn new skills by doing. We improve those skills by doing more. These fundamental facts are true of even the most demanding skills.

%% the importance of doing %%

The knowledge required to ride a bicycle can’t be fully captured in words and conveyed to others. We need “tacit knowledge,” the sort we only get from bruising experience. To learn to ride a bicycle, we must try to ride one. It goes badly at first. You fall to one side, you fall to the other. But keep at it and with practice it becomes effortless—although if you had to explain how to stay upright, so they can skip the ordeal you just went through, you would succeed no better than Polanyi.

%% on tacit knowledge and bruising experience%%

But not all practice improves skill. It needs to be informed practice. You need to know which mistakes to look out for—and which best practices really are best.

Effective practice also needs to be accompanied by clear and timely feedback.

That is essential. To learn from failure, we must know when we fail. The baby who flops backward does.

%%good, clear and timely feedback from practice is needed%%

**Unfortunately, most forecasters do not get the high-quality feedback that helps meteorologists and bridge players improve. There are two main reasons why. Ambiguous language is a big one.** As we saw in chapter 3, vague terms like “probably” and “likely” make it impossible to judge forecasts. When a forecaster says something could or might or may happen, she could or might or may be saying almost anything. The same is true of countless other terms—like Steve Ballmer’s reference to “significant market share”—that may sound precise but on close inspection prove as fuzzy as fog. Even an impartial observer would struggle to extract meaningful feedback from vague forecasts, but often the judge is the forecaster herself. That makes the problem even worse.

**The second big barrier to feedback is time lag.** When forecasts span months or years, the wait for a result allows the flaws of memory to creep in. You know how you feel now about the future. But as events unfold, will you be able to recall your forecast accurately? There is a good chance you won’t. Not only will you have to contend with ordinary forgetfulness, you are likely to be afflicted by what psychologists call hindsight bias.

%% two reasons why many forecasters dont get good feedback %%

Forecasters who use ambiguous language and rely on flawed memories to retrieve old forecasts don’t get clear feedback, which makes it impossible to learn from experience.

%%importance of clear language%%

When Tim Minto forecast Syrian refugee flows in 2014, he got a Brier score of 0.07. That is clear, precise, and meaningful, an excellent result, the forecasting equivalent of a nothing-but-net free throw. Less happily, Tim’s forecast on whether Shinzo Abe would visit the Yakusuni Shrine scored a 1.46, which was like throwing the ball into the trash can at the back of the gymnasium. And Tim knew it. There was no ambiguity in the language to hide behind, no way hindsight bias could subtly fool him into believing his forecast wasn’t so bad. Tim blew it and he knew it, which gave him the chance to learn. By the way, there are no shortcuts.

%%admit failure when off%%

Whenever a question closes, it’s obvious that superforecasters—in sharp contrast to Carol Dweck’s fixed-mindset study subjects—are as keen to know how they can do better as they are to know how they did.

%% be curious regarding improving %%

Question after question, month after month, she kept at it. That’s grit. It’s also why I wouldn’t be at all surprised to see her ultimately become a superforecaster.

%%grit is important%%

In philosophic outlook, they tend to be:

- CAUTIOUS: Nothing is certain 
- HUMBLE: Reality is infinitely complex
- NONDETERMINISTIC: What happens is not meant to be and does not have to happen

In their abilities and thinking styles, they tend to be: 

- ACTIVELY OPEN-MINDED: Beliefs are hypotheses to be tested, not treasures to be protected
- INTELLIGENT AND KNOWLEDGEABLE, WITH A “NEED FOR COGNITION”: Intellectually curious, enjoy puzzles and mental challenges
- REFLECTIVE: Introspective and self-critical
- NUMERATE: Comfortable with numbers In their methods of forecasting they tend to be:
- PRAGMATIC: Not wedded to any idea or agenda
- ANALYTICAL: Capable of stepping back from the tip-of-your-nose perspective and considering other views
- DRAGONFLY-EYED: Value diverse views and synthesize them into their own
- PROBABILISTIC: Judge using many grades of maybe
- THOUGHTFUL UPDATERS: When facts change, they change their minds
- GOOD INTUITIVE PSYCHOLOGISTS: Aware of the value of checking thinking for cognitive and emotional biases

In their work ethic, they tend to have:
- A GROWTH MINDSET: Believe it’s possible to get better
- GRIT: Determined to keep at it however long it takes

I paint with a broad brush here. Not every attribute is equally important. The strongest predictor of rising into the ranks of superforecasters is perpetual beta, the degree to which one is committed to belief updating and self-improvement. It is roughly three times as powerful a predictor as its closest rival, intelligence.

%% Traits for superforecasters%%
## Chapter 9 - Superteams

Groups that get along too well don’t question assumptions or confront uncomfortable facts. So everyone agrees, which is pleasant, and the fact that everyone agrees is tacitly taken to be proof the group is on the right track. We can’t all be wrong, can we?

But loss of independence isn’t inevitable in a group, as JFK’s team showed during the Cuban missile crisis. If forecasters can keep questioning themselves and their teammates, and welcome vigorous debate, the group can become more than the sum of its parts.

%% Consensus is not good for forecasting in groups%%

The risks were obvious. Tell someone they’re exceptionally good at something and they may start taking their superiority for granted. Surround them with others who are similarly accomplished, tell them how special they are, and egos may swell even more. Rather than spur a superforecaster to take his game to the next level, it might make him so sure of himself that he is tempted to think his judgment must be right because it is his judgment. This is a familiar paradox: success can lead to acclaim that can undermine the habits of mind that produced the success. Such hubris often afflicts highly accomplished individuals. In business circles, it is called CEO disease.

%% Success can make us question less, also not good for forecasting%%

Experience helped. Seeing this “dancing around,” people realized that excessive politeness was hindering the critical examination of views, so they made special efforts to assure others that criticism was welcome. “Everybody has said, ‘I want push-back from you if you see something I don’t,’ ” said Rosenthal. That made a difference. So did offering thanks for constructive criticism. Gradually, the dancing around diminished.

%% Constructive criticism%%

In key ways, superteams resembled the best surgical teams identified by Harvard’s Amy Edmondson, in which the nurse doesn’t hesitate to tell the surgeon he left a sponge behind the pancreas because she knows it is “psychologically safe” to correct higher-ups. Edmondson’s best teams had a shared purpose. So did our superteams. One sign of that was linguistic: they said “our” more than “my.”

%% "Our" vs "my" focus%%

A group of open-minded people who don’t care about one another will be less than the sum of its open-minded parts. A group of opinionated people who engage one another in pursuit of the truth will be more than the sum of its opinionated parts. All this brings us to the final feature of winning teams: the fostering of a culture of sharing. My Wharton colleague Adam Grant categorizes people as “givers,” “matchers,” and “takers.”

Marty Rosenthal is a giver. He wasn’t indiscriminately generous with his time and effort. He was generous in a deliberate effort to change the behavior of others for the benefit of all. Although Marty didn’t know Grant’s work, when I described it to him, he said, “You got it.” There are lots more givers on the superteams. Doug Lorch distributed programming tools, which got others thinking about creating and sharing their own. Tim Minto contributed an analysis that showed how to make valuable automatic tweaks to forecasts with the passage of time. All are givers.

%% Givers are important, especially groups with many of them can have great results %%
## Chapter 10 - The Leader's Dilemma

This looks like a serious dilemma. Leaders must be forecasters and leaders but it seems that what is required to succeed at one role may undermine the other. Fortunately, the contradiction between being a superforecaster and a superleader is more apparent than real. In fact, the superforecaster model can help make good leaders superb and the organizations they lead smart, adaptable, and effective.

%% The divide between leader and forecaster traits doesn't have to be incompatible%%

Imagine a top-down, command-and-control military unit approaching a town. A captain is ordered to take the town. How? Approach from the southwest, skirt the factory on the outskirts, seize the canal bridge, then occupy the town hall. Why? Not his business.

In the Wehrmacht, by contrast, a captain would be told to take the town. How? That’s up to him. Why? Because his superior has been ordered to prevent enemy reinforcements from reaching the region on the other side of the town and taking the town will sever a key road. Thanks to Auftragstaktik, the captain can devise a plan for capturing the town that takes into account the circumstances he encounters, not those headquarters expects him to encounter.

%% Top-down vs more shared decision power%%

“it must be admitted that the Germany Army of World War II was, man for man, one of the most effective fighting forces ever seen.”14 Ultimately, the Wehrmacht failed. In part, it was overwhelmed by its enemies’ superior resources. But it also made blunders—often because its commander in chief, Adolf Hitler, took direct control of operations in violation of Helmuth von Moltke’s principles, nowhere with more disastrous effect than during the invasion of Normandy.

%% More shared decision power makes for more effective actions%%

But Petraeus sees the divide between doers and thinkers as a false dichotomy. Leaders must be both. “The bold move is the right move except when it’s the wrong move,” he says. A leader “needs to figure out what’s the right move and then execute it boldly.”23 That’s the tension between deliberation and implementation that Moltke emphasized and Petraeus balanced in Iraq.

%% Leaders must be both doers and thinkers%%

“We let our people know what we want them to accomplish. But—and it is a very big ‘but’—we do not tell them how to achieve those goals.”25 That is a near-perfect summary of “mission command.”

%% Freedom with a clear mission%%

The humility required for good judgment is not self-doubt—the sense that you are untalented, unintelligent, or unworthy. It is intellectual humility. It is a recognition that reality is profoundly complex, that seeing things clearly is a constant struggle, when it can be done at all, and that human judgment must therefore be riddled with mistakes.

%% Intelligent humility is very important for good judgment%%
## Chapter 11 - Are They Really So Super?

Our advanced training guidelines urge forecasters to mentally tinker with “the question asked” and explore how their answers to a timing question might change if the cutoff date were six months out instead of twelve, or if the target price for oil were 10% lower, or some other relevant variation. Running such “thought experiments” inside your head is a good way to stress-test the adequacy of your mental model of the problem and ensure you are scope sensitive. But the truth is superforecasters were talking about problems of scope insensitivity—although they didn’t use that term—before we started to study it, and their thinking helped inform our training guidelines as much as our training guidelines informed their thinking.

%%Tinkering with the question and understanding the scope of a question is important%%

Dispelling the dichotomy requires putting the beguiling black swan metaphor under the analytic microscope. What exactly is a black swan? The stringent definition is something literally inconceivable before it happens. Taleb has implied as much on occasion. If so, many events dubbed black swans are actually gray.

%%Black swans are maybe just gray?%%

Consider the 9/11 terrorist attacks, the prototypic black swan in which one dazzling sunny morning in September, a bolt from the blue changed history. But 9/11 was not unimaginable. In 1994 a plot to hijack a jet and crash it into the Eiffel Tower was broken up. In 1998 the US Federal Aviation Administration examined a scenario in which terrorists hijacked FedEx cargo planes and crashed them into the World Trade Center. The danger was so well known in security circles that in August 2001, a government official asked Louise Richardson, a Harvard terrorism expert, why no terrorist group had ever used an airplane as a flying bomb. “My answer was far from helpful,” she later wrote. “I said that the tactic was very much under consideration and I suspected that some terrorist groups would use it sooner rather than later.”5 Other events that have been called black swans—such as the outbreak of World War I, which was preceded by more than a decade of fretting about the danger of war among the great powers—also fail the unimaginability test. If black swans must be inconceivable before they happen, a rare species of event suddenly becomes a lot rarer.

%% 9/11 wasnt the first time anyone hijacked something to crash it, so was it really a black swan? %%

If forecasters make hundreds of forecasts that look out only a few months, we will soon have enough data to judge how well calibrated they are. But by definition, “highly improbable” events almost never happen. If we take “highly improbable” to mean a 1% or 0.1% or 0.0001% chance of an event, it may take decades or centuries or millennia to pile up enough data. And if these events have to be not only highly improbable but also impactful, the difficulty multiplies. So the first-generation IARPA tournament tells us nothing about how good superforecasters are at spotting gray or black swans. They may be as clueless as anyone else—or astonishingly adept. We don’t know, and shouldn’t fool ourselves that we do.

%% Some events are so unlikely we wont see them,  it may take decades or centuries or millennia to pile up enough data%%

Three days after terrorists flew jets into the World Trade Center and the Pentagon, the US government demanded that the Taliban rulers of Afghanistan hand over Osama bin Laden and other al-Qaeda terrorists. The Taliban said they would comply if the US government produced satisfactory evidence of al-Qaeda’s guilt. The United States prepared for invasion. Still the Taliban refused to hand over bin Laden. Finally, almost a month after 9/11, the United States attacked. Today, when we refer to the black swan of 9/11, we mean the attacks plus the consequences, which include the invasion of Afghanistan. But that sequence of events could arguably have gone differently. Bin Laden and al-Qaeda were Arabic-speaking foreigners in Afghanistan and the Taliban could have decided that sheltering them just wasn’t worth incurring the wrath of the world’s sole superpower right when they were on the cusp of finally destroying the Northern Alliance, their hated rivals. Or, sensing imminent extradition, bin Laden and his followers could have fled to Pakistan, Somalia, or Yemen. You could imagine a scenario in which there was no invasion of Afghanistan, or no ten-year hunt for Osama bin Laden. And we would then see 9/11 differently—a tragedy, certainly, but not the opening shot in a series of wars that dominated a decade.

%% In retrospect we put all things together that happen in a chain of events after the planes, creating the black swan we speak of today, maybe a chain of very unlikely events that conspired and some of them could have been forecasted in isolation%%


![[Skärmavbild 2026-01-14 kl. 23.43.26.png|300]]
Now comes the hardest-to-grasp part of Taleb’s view of the world. He posits that historical probabilities—all the possible ways the future could unfold—are distributed like wealth, not height. That means our world is vastly more volatile than most of us realize and we are at risk of grave miscalculations.

%%The point is that we are more volatile then is to maybe be belived%%

## Chapter 12 - What's Next?

Just as we now expect a pill to have been tested in peer-reviewed experiments before we swallow it, we will expect forecasters to establish the accuracy of their forecasting with rigorous testing before we heed their advice. And forecasters themselves will realize, as Dan Drezner did, that these higher expectations will ultimately benefit them, because it is only with the clear feedback that comes from rigorous testing that they can improve their foresight. It could be huge—an “evidence-based forecasting” revolution similar to the “evidence-based medicine” revolution, with consequences every bit as significant.

%%Asking for better forecasting processes%%

It was an extreme illustration of a problem I swept under the rug in chapter 1, when I said the exclusive goal of forecasting should be accuracy and that would be the sole concern in this book. In reality, accuracy is often only one of many goals. Sometimes it is irrelevant.

Like many hardball operators before and since, Vladimir Lenin insisted politics, defined broadly, was nothing more than a struggle for power, or as he memorably put it, “kto, kogo?” That literally means “who, whom” and it was Lenin’s shorthand for “Who does what to whom?” Arguments and evidence are lovely adornments but what matters is the ceaseless contest to be the kto, not the kogo.6 It follows that the goal of forecasting is not to see what’s coming. It is to advance the interests of the forecaster and the forecaster’s tribe. Accurate forecasts may help do that sometimes, and when they do accuracy is welcome, but it is pushed aside if that’s what the pursuit of power requires.

You don’t have to be a Marxist-Leninist to concede that Lenin had a point. Self and tribe matter. If forecasting can be co-opted to advance their interests, it will be. From this perspective, there is no need to reform and improve forecasting, and it will not change, because it is already serving its primary purpose well.

%%The goal of the actors matter a lot, sometimes accuracy is not the goal%%

Recall the wrong-side-of-maybe fallacy that leads people to conclude a forecast of “There is a 70% chance an event will happen” is wrong if the event doesn’t happen. It was a big reason why Sherman Kent’s modest proposal to attach numerical ranges to forecasts went nowhere. Use the number and you risk being unfairly blamed. Stick with phrases as fuzzy as a puff of smoke and you are safe.

%%Being percentage based risks blame%%

I actually feel that there is a lot of truth in this perspective. Far too many people treat numbers like sacred totems offering divine insight. The truly numerate know that numbers are tools, nothing more, and their quality can range from wretched to superb. A crude version of Codman’s End Results System that simply tracked patient survival might result in a hospital boasting that 100% of its patients live—without mentioning that the hospital achieves this bragging right by turning away the sickest patients. Numbers must be constantly scrutinized and improved, which can be an unnerving process because it is unending. Progressive improvement is attainable. Perfection is not.15

%%Numbers are tool, not divine insight%%

Seen that way, it’s obvious that the big question is composed of many small questions. One is “Will North Korea test a rocket?” If it does, it will escalate the conflict a little. If it doesn’t, it could cool things down a little. That one tiny question doesn’t nail down the big question, but it does contribute a little insight. And if we ask many tiny-but-pertinent questions, we can close in on an answer for the big question. Will North Korea conduct another nuclear test? Will it rebuff diplomatic talks on its nuclear program? Will it fire artillery at South Korea? Will a North Korean ship fire on a South Korean ship? The answers are cumulative. The more yeses, the likelier the answer to the big question is “This is going to end badly.”

![[Skärmavbild 2026-01-15 kl. 09.26.29.png|400]]

%%Think and de-construct questions, are they big or small?%%

As J. M. Keynes is reputed to have said but did not, **“When the facts change, I change my mind. What do you do, sir?”**

%%On changing your mind QUOTE%%